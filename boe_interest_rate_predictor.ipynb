{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Bank of England (BoE) Interest Rate Predictor\"\n",
    "author: 'Yuyao Bai'\n",
    "format: html\n",
    "self-contained: true\n",
    "jupyter: python3\n",
    "engine: jupyter\n",
    "editor:\n",
    "  render-on-save: true\n",
    "  preview: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"A2u2nh\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.5.2/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"A2u2nh\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"A2u2nh\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# libraries for dataframe and data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# libraries for plotting\n",
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "from lets_plot.plot import gggrid\n",
    "import matplotlib.pyplot as plt\n",
    "#libraries for table formatting\n",
    "from pytablewriter import MarkdownTableWriter\n",
    "# libraries for data exploration (e.g missing values)\n",
    "import missingno as msno\n",
    "import sweetviz as sv\n",
    "#libraries for pre-processing the data \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures, MinMaxScaler, PowerTransformer\n",
    "#libraries for models\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "import miceforest as mf\n",
    "#libraries for train/test split\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "#libraries for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "# libraries for metric definition (model evaluation)\n",
    "from sklearn.metrics import mean_absolute_percentage_error, balanced_accuracy_score, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, average_precision_score, make_scorer, classification_report, fbeta_score, precision_recall_curve\n",
    "# libraries for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Processing  \n",
    "#### 1.1.1 BoE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rate</th>\n",
       "      <th>rate_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-05-06</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-06-06</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-07-10</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-08-07</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-11-06</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2023-06-22</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Rate  rate_change\n",
       "0   1997-05-06  6.25            1\n",
       "1   1997-06-06  6.50            1\n",
       "2   1997-07-10  6.75            1\n",
       "3   1997-08-07  7.00            1\n",
       "4   1997-11-06  7.25            1\n",
       "..         ...   ...          ...\n",
       "63  2023-06-22  5.00            1\n",
       "64  2023-08-03  5.25            1\n",
       "65  2024-08-01  5.00           -1\n",
       "66  2024-11-07  4.75           -1\n",
       "67  2025-02-06  4.50           -1\n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/BoE_interest_rates.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68 entries, 0 to 67\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Date         68 non-null     object \n",
      " 1   Rate         68 non-null     float64\n",
      " 2   rate_change  68 non-null     int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure observations are ordered in the correct order of date \n",
    "df = df.sort_values(by='Date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that rate setting events happen every once in a while, we calculate the difference in days between consecutive rate setting events. The output below shows that the rate setting events are indeed spaced out at irregular intervals, with the minimum difference being 8 days and the maximum being 2709 days. This suggests that the **rate setting events are not scheduled at fixed intervals**, which is common in central bank monetary policy decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date_diff\n",
       "28 days      8\n",
       "49 days      7\n",
       "42 days      6\n",
       "35 days      5\n",
       "63 days      5\n",
       "364 days     4\n",
       "91 days      4\n",
       "56 days      3\n",
       "154 days     2\n",
       "455 days     2\n",
       "119 days     2\n",
       "98 days      2\n",
       "273 days     1\n",
       "2709 days    1\n",
       "29 days      1\n",
       "181 days     1\n",
       "8 days       1\n",
       "637 days     1\n",
       "587 days     1\n",
       "31 days      1\n",
       "16 days      1\n",
       "34 days      1\n",
       "84 days      1\n",
       "70 days      1\n",
       "57 days      1\n",
       "90 days      1\n",
       "126 days     1\n",
       "210 days     1\n",
       "47 days      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that rate setting events happen every once in a while\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date_diff'] = df['Date'].diff()\n",
    "df['Date_diff'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Economic Indicators Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_df = pd.read_csv('data/economic_indicators_interest_rate_setting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>CCI</th>\n",
       "      <th>Unemployment rate (aged 16 and over, seasonally adjusted): %</th>\n",
       "      <th>10-year-gilt-yield</th>\n",
       "      <th>CPIH MONTHLY RATE 00: ALL ITEMS 2015=100</th>\n",
       "      <th>Gross Value Added - Monthly (Index 1dp) :CVM SA</th>\n",
       "      <th>Monthly average Spot exchange rate, Sterling into US$              [a]             XUMAGBD</th>\n",
       "      <th>Monthly average Spot exchange rates, Sterling into Euro              [a]             XUMASER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>102.2504</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5552</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.6031</td>\n",
       "      <td>0.7376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-02-01</td>\n",
       "      <td>102.5327</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.1962</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.7192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-03-01</td>\n",
       "      <td>102.6905</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.4544</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.6</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.7172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-04-01</td>\n",
       "      <td>102.7900</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.6380</td>\n",
       "      <td>0.4</td>\n",
       "      <td>63.3</td>\n",
       "      <td>0.6137</td>\n",
       "      <td>0.7022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-05-01</td>\n",
       "      <td>102.9294</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.1681</td>\n",
       "      <td>0.4</td>\n",
       "      <td>62.7</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       CCI  \\\n",
       "0  1997-01-01  102.2504   \n",
       "1  1997-02-01  102.5327   \n",
       "2  1997-03-01  102.6905   \n",
       "3  1997-04-01  102.7900   \n",
       "4  1997-05-01  102.9294   \n",
       "\n",
       "   Unemployment rate (aged 16 and over, seasonally adjusted): %  \\\n",
       "0                                                7.5              \n",
       "1                                                7.3              \n",
       "2                                                7.2              \n",
       "3                                                7.2              \n",
       "4                                                7.2              \n",
       "\n",
       "   10-year-gilt-yield  CPIH MONTHLY RATE 00: ALL ITEMS 2015=100  \\\n",
       "0              7.5552                                      -0.3   \n",
       "1              7.1962                                       0.2   \n",
       "2              7.4544                                       0.2   \n",
       "3              7.6380                                       0.4   \n",
       "4              7.1681                                       0.4   \n",
       "\n",
       "   Gross Value Added - Monthly (Index 1dp) :CVM SA  \\\n",
       "0                                             62.0   \n",
       "1                                             62.5   \n",
       "2                                             62.6   \n",
       "3                                             63.3   \n",
       "4                                             62.7   \n",
       "\n",
       "   Monthly average Spot exchange rate, Sterling into US$              [a]             XUMAGBD  \\\n",
       "0                                             0.6031                                            \n",
       "1                                             0.6156                                            \n",
       "2                                             0.6226                                            \n",
       "3                                             0.6137                                            \n",
       "4                                             0.6122                                            \n",
       "\n",
       "   Monthly average Spot exchange rates, Sterling into Euro              [a]             XUMASER  \n",
       "0                                             0.7376                                             \n",
       "1                                             0.7192                                             \n",
       "2                                             0.7172                                             \n",
       "3                                             0.7022                                             \n",
       "4                                             0.7034                                             "
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'CCI',\n",
       "       'Unemployment rate (aged 16 and over, seasonally adjusted): %',\n",
       "       '10-year-gilt-yield', 'CPIH MONTHLY RATE 00: ALL ITEMS 2015=100',\n",
       "       'Gross Value Added - Monthly (Index 1dp) :CVM SA',\n",
       "       'Monthly average Spot exchange rate, Sterling into US$              [a]             XUMAGBD',\n",
       "       'Monthly average Spot exchange rates, Sterling into Euro              [a]             XUMASER'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will rename the columns above to make them more concise. Additionally, similar to the BoE dataset, we convert the date columns to datetime format and ensure that the data is correctly sorted by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "column_names = {\n",
    "    'Date': 'date',\n",
    "    'CCI': 'cci',\n",
    "    'Unemployment rate (aged 16 and over, seasonally adjusted): %': 'unemployment_rate',\n",
    "    '10-year-gilt-yield': 'gilt_yield',\n",
    "    'CPIH MONTHLY RATE 00: ALL ITEMS 2015=100': 'cpih',\n",
    "    'Gross Value Added - Monthly (Index 1dp) :CVM SA': 'gva', \n",
    "    'Monthly average Spot exchange rate, Sterling into US$              [a]             XUMAGBD': 'er_gbp_usd',\n",
    "    'Monthly average Spot exchange rates, Sterling into Euro              [a]             XUMASER': 'er_gbp_eur'\n",
    "}\n",
    "\n",
    "indicators_df = indicators_df.rename(columns=column_names)\n",
    "\n",
    "# convert the date column to datetime\n",
    "indicators_df['date'] = pd.to_datetime(indicators_df['date'])\n",
    "\n",
    "# make sure observations are ordered in the correct order of date\n",
    "indicators_df = indicators_df.sort_values(by='date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cci</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>gilt_yield</th>\n",
       "      <th>cpih</th>\n",
       "      <th>gva</th>\n",
       "      <th>er_gbp_usd</th>\n",
       "      <th>er_gbp_eur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>102.2504</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5552</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.6031</td>\n",
       "      <td>0.7376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-02-01</td>\n",
       "      <td>102.5327</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.1962</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0.6156</td>\n",
       "      <td>0.7192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-03-01</td>\n",
       "      <td>102.6905</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.4544</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.6</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.7172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-04-01</td>\n",
       "      <td>102.7900</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.6380</td>\n",
       "      <td>0.4</td>\n",
       "      <td>63.3</td>\n",
       "      <td>0.6137</td>\n",
       "      <td>0.7022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-05-01</td>\n",
       "      <td>102.9294</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.1681</td>\n",
       "      <td>0.4</td>\n",
       "      <td>62.7</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       cci  unemployment_rate  gilt_yield  cpih   gva  er_gbp_usd  \\\n",
       "0 1997-01-01  102.2504                7.5      7.5552  -0.3  62.0      0.6031   \n",
       "1 1997-02-01  102.5327                7.3      7.1962   0.2  62.5      0.6156   \n",
       "2 1997-03-01  102.6905                7.2      7.4544   0.2  62.6      0.6226   \n",
       "3 1997-04-01  102.7900                7.2      7.6380   0.4  63.3      0.6137   \n",
       "4 1997-05-01  102.9294                7.2      7.1681   0.4  62.7      0.6122   \n",
       "\n",
       "   er_gbp_eur  \n",
       "0      0.7376  \n",
       "1      0.7192  \n",
       "2      0.7172  \n",
       "3      0.7022  \n",
       "4      0.7034  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Merging Datasets\n",
    "Now, we want to assign to each row of ```df_baseline```, values of economic indicators from the last quarter i.e the average of each indicator for the last three months up to the date of the rate setting event. For example, if the rate setting event is on 06/05/1997, we will average data for GDP for May 1997, April 1997, and March 1997, and do the same separately for the other indicators i.e exchange rates, 10-year gilt yield, unemployment rates, CPIH, and CCI. A function ```get_last_quarter_values``` is defined to achieve this.  \n",
    "  \n",
    "By taking the aggregate of the economic indicators for the last quarter, we aim to capture the economic conditions leading up to the rate setting event and assess how these conditions influence the Bank of England's monetary policy decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rate</th>\n",
       "      <th>rate_change</th>\n",
       "      <th>Date_diff</th>\n",
       "      <th>cci</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>gilt_yield</th>\n",
       "      <th>cpih</th>\n",
       "      <th>gva</th>\n",
       "      <th>er_gbp_usd</th>\n",
       "      <th>er_gbp_eur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-05-06</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>102.803300</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.420167</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>62.866667</td>\n",
       "      <td>0.616167</td>\n",
       "      <td>0.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-06-06</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1</td>\n",
       "      <td>31 days</td>\n",
       "      <td>102.885733</td>\n",
       "      <td>7.233333</td>\n",
       "      <td>7.315967</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.611333</td>\n",
       "      <td>0.698467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-07-10</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1</td>\n",
       "      <td>34 days</td>\n",
       "      <td>102.910667</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.122533</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>63.033333</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.683067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-08-07</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1</td>\n",
       "      <td>28 days</td>\n",
       "      <td>102.891300</td>\n",
       "      <td>7.066667</td>\n",
       "      <td>7.096267</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>0.610233</td>\n",
       "      <td>0.670233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-11-06</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>91 days</td>\n",
       "      <td>102.958967</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>6.652167</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>63.933333</td>\n",
       "      <td>0.609667</td>\n",
       "      <td>0.679933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2023-06-22</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>42 days</td>\n",
       "      <td>97.089057</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.992767</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>100.466667</td>\n",
       "      <td>0.798900</td>\n",
       "      <td>0.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1</td>\n",
       "      <td>42 days</td>\n",
       "      <td>97.734747</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>4.444300</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>100.533333</td>\n",
       "      <td>0.785067</td>\n",
       "      <td>0.858533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>364 days</td>\n",
       "      <td>99.977790</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>4.175300</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>101.300000</td>\n",
       "      <td>0.785267</td>\n",
       "      <td>0.848567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-1</td>\n",
       "      <td>98 days</td>\n",
       "      <td>99.141300</td>\n",
       "      <td>4.366667</td>\n",
       "      <td>4.173667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>101.200000</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-1</td>\n",
       "      <td>91 days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Rate  rate_change Date_diff         cci  unemployment_rate  \\\n",
       "0  1997-05-06  6.25            1       NaT  102.803300           7.200000   \n",
       "1  1997-06-06  6.50            1   31 days  102.885733           7.233333   \n",
       "2  1997-07-10  6.75            1   34 days  102.910667           7.200000   \n",
       "3  1997-08-07  7.00            1   28 days  102.891300           7.066667   \n",
       "4  1997-11-06  7.25            1   91 days  102.958967           6.600000   \n",
       "..        ...   ...          ...       ...         ...                ...   \n",
       "63 2023-06-22  5.00            1   42 days   97.089057           4.166667   \n",
       "64 2023-08-03  5.25            1   42 days   97.734747           4.233333   \n",
       "65 2024-08-01  5.00           -1  364 days   99.977790           4.166667   \n",
       "66 2024-11-07  4.75           -1   98 days   99.141300           4.366667   \n",
       "67 2025-02-06  4.50           -1   91 days         NaN                NaN   \n",
       "\n",
       "    gilt_yield      cpih         gva  er_gbp_usd  er_gbp_eur  \n",
       "0     7.420167  0.333333   62.866667    0.616167    0.707600  \n",
       "1     7.315967  0.333333   63.000000    0.611333    0.698467  \n",
       "2     7.122533  0.133333   63.033333    0.606367    0.683067  \n",
       "3     7.096267  0.133333   63.266667    0.610233    0.670233  \n",
       "4     6.652167  0.133333   63.933333    0.609667    0.679933  \n",
       "..         ...       ...         ...         ...         ...  \n",
       "63    3.992767  0.666667  100.466667    0.798900    0.869900  \n",
       "64    4.444300  0.100000  100.533333    0.785067    0.858533  \n",
       "65    4.175300  0.200000  101.300000    0.785267    0.848567  \n",
       "66    4.173667  0.300000  101.200000    0.769200    0.836300  \n",
       "67         NaN       NaN         NaN         NaN         NaN  \n",
       "\n",
       "[68 rows x 11 columns]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators_df_baseline = indicators_df.copy()\n",
    "df_baseline = df.copy()\n",
    "\n",
    "indicators = ['cci', 'unemployment_rate', 'gilt_yield', 'cpih', 'gva', 'er_gbp_usd', 'er_gbp_eur']\n",
    "\n",
    "def get_last_quarter_values(date, indicators_df_baseline, indicators):\n",
    "    # get the last quarter values\n",
    "    last_quarter = date - pd.DateOffset(months=3)\n",
    "    last_quarter_values = indicators_df_baseline.query('date < @date & date >= @last_quarter')[indicators].mean()\n",
    "    \n",
    "    return last_quarter_values\n",
    "\n",
    "df_baseline[indicators] = df_baseline['Date'].apply(lambda x: get_last_quarter_values(x, indicators_df_baseline, indicators))\n",
    "\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                 0\n",
       "Rate                 0\n",
       "rate_change          0\n",
       "Date_diff            1\n",
       "cci                  1\n",
       "unemployment_rate    1\n",
       "gilt_yield           1\n",
       "cpih                 1\n",
       "gva                  1\n",
       "er_gbp_usd           1\n",
       "er_gbp_eur           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```\"Date_diff\"``` column created in 1.1.1 is dropped as it is no longer needed. We also drop the last row of the dataset as it contains NaN values due to the lack of data for the last quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Date_diff column\n",
    "df_baseline = df_baseline.drop('Date_diff', axis=1)\n",
    "\n",
    "# drop the last row (since we lack any information about the economic indicators for this row)\n",
    "df_baseline = df_baseline.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ```df_baseline``` does not contain any missing values and is ready for modelling in the next section.\n",
    "\n",
    "### 1.2 Baseline logistic regression model  \n",
    "In this part, we'll focus on predicting ```\"rate_change\"``` contained in the Bank of England interest rates dataset. A logistic regression model will be used as the baseline model to predict whether the Bank of England will increase or decrease the interest rates at the next rate setting event.   \n",
    "  \n",
    "We first note that the target variable, ```\"rate_change\"```, is defined as follows:\n",
    "* 1: if the Bank of England increases the interest rates at a rate setting event  \n",
    "* -1: if the Bank of England decreases the interest rates at a rate setting event   \n",
    "  \n",
    "Since sklearn's LogisticRegression expects labels to be in {0, 1}, we will do a simple encoding of the target variable by mapping -1 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the rate_change column\n",
    "df_baseline['rate_change'] = (df_baseline['rate_change'] == 1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to split the data into training and test sets, with 70% of the years for the training set. Since we have already ensured in 1.1.1 and 1.1.2 that the data is correctly sorted by date, we can use a `train_test_split` with `shuffle=False` to maintain the order of the data. This is important as we are dealing with time series data, and we want to avoid data leakage from the future into the past, i.e. we want to train the model on past data and test it on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split (70/30), noting that the data is temporal\n",
    "X = df_baseline[indicators]\n",
    "y = df_baseline['rate_change']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we standardise the features using the `StandardScaler` to ensure that all features are on the same scale. This is important for logistic regression as it is sensitive to the scale of the features. The scaler is fit on the training set and then applied to both the training and test sets so as to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features, fitting the scaler on the training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "# fit this instance to the training set\n",
    "_ = logit.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on the training and test sets\n",
    "y_train_pred = logit.predict(X_train_scaled)\n",
    "y_test_pred = logit.predict(X_test_scaled)\n",
    "\n",
    "y_train_probas = logit.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_probas = logit.predict_proba(X_test_scaled)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Baseline Logistic Regression Model Evaluation  \n",
    "Let us look at the class distribution of the overall dataset, as well as the training and test sets to decide an appropriate evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rate_change\n",
       "1    0.537313\n",
       "0    0.462687\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline['rate_change'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Distribution: rate_change\n",
      "0    0.565217\n",
      "1    0.434783\n",
      "Name: proportion, dtype: float64\n",
      "Test Class Distribution: rate_change\n",
      "1    0.761905\n",
      "0    0.238095\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Class Distribution: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test Class Distribution: {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the overall dataset is relatively balanced, when we look at the distribution of the target variable in the training and test sets, we see that there is **moderate class imbalance in the test set**, with more instances of interest rate increases (76.2%) than decreases (23.8%). Hence, we need to take this into account when choosing a metric to evaluate the model's performance.  \n",
    "  \n",
    "> As an aside, we note that the class imbalance in the test set is nothing surprising when we consider the real-world context of the Bank of England's interest rate decisions. The high proportion of rate hikes reflects the BoE's sustained monetary tightening strategy from late 2021 through 2023, as they aggressively raised rates to combat inflation. This hiking cycle, the most rapid in decades, was driven by post-pandemic supply chain disruptions, labour shortages, and energy price shocks from the Ukraine war, forcing the MPC to take a hawkish stance to curb inflationary pressures despite recession risks. As a result, our test set is skewed towards rate hikes.\n",
    "\n",
    "Given the class imbalance, I choose to use the **Area Under the Precision-Recall Curve (AUC-PR)** as the evaluation metric. AUC-PR is well-suited for imbalanced datasets because it focuses on the model's ability to correctly identify interest rate increases (class 1) while balancing precision and recall. We want to balance the trade-off between precision (the proportion of correctly predicted interest rate increases among all predicted interest rate increases) and recall (the proportion of correctly predicted interest rate increases among all actual interest rate increases)  \n",
    "  \n",
    "AUC-PR is also more informative than the F1-score. The F1-score is a single classification threshold (0.5 is the default), whereas the AUC-PR considers all possible classification thresholds, providing a better overall picture of how well the model distinguishes between rate increases and decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC-PR: 0.972\n",
      "Test AUC-PR: 0.734\n"
     ]
    }
   ],
   "source": [
    "train_auc_pr = average_precision_score(y_train, y_train_probas)\n",
    "test_auc_pr = average_precision_score(y_test, y_test_probas)\n",
    "\n",
    "print(f'Train AUC-PR: {train_auc_pr:.3f}')\n",
    "print(f'Test AUC-PR: {test_auc_pr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**  \n",
    "The model classifies the training set with an AUC-PR of 0.972, indicating that the model performs very well in distinguishing between rate increases and decreases in the training set. The AUC-PR for the test set is 0.734, showing signs of overfitting, as the model's performance drops when applied to unseen data. While these are relatively high AUC-PR values, we need to err on the side of caution when interpreting the results as we have not considered publication lags in the economic indicators data (detailed explanation in section 1.3). Hence, we cannot immediately conclude that the baseline model performs relatively well at predicting the Bank of England's interest rate decisions using the economic indicators.  \n",
    "  \n",
    "Let's also plot the confusion matrices for the training and test sets to easily visualise how the model performs in terms of true positives, false positives, true negatives, and false negatives. In the context of this problem:\n",
    "* True Positive (TP): The model correctly predicts an interest rate increase\n",
    "* True Negative (TN): The model correctly predicts an interest rate decrease\n",
    "* False Positive (FP): The model incorrectly predicts an interest rate increase when it is actually a decrease\n",
    "* False Negative (FN): The model incorrectly predicts an interest rate decrease when it is actually an increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"ncmKCU\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "   \n",
       "   (function() {\n",
       "   // ----------\n",
       "   \n",
       "   var containerDiv = document.getElementById(\"ncmKCU\");\n",
       "   var observer = new ResizeObserver(function(entries) {\n",
       "       for (let entry of entries) {\n",
       "           var width = containerDiv.clientWidth\n",
       "           if (entry.contentBoxSize && width > 0) {\n",
       "           \n",
       "               // Render plot\n",
       "               if (observer) {\n",
       "                   observer.disconnect();\n",
       "                   observer = null;\n",
       "               }\n",
       "\n",
       "               var plotSpec={\n",
       "\"kind\":\"subplots\",\n",
       "\"layout\":{\n",
       "\"ncol\":2.0,\n",
       "\"nrow\":1.0,\n",
       "\"name\":\"grid\"\n",
       "},\n",
       "\"figures\":[{\n",
       "\"data\":{\n",
       "\"Actual\":[\"Actual 0\",\"Actual 1\",\"Actual 0\",\"Actual 1\"],\n",
       "\"Predicted\":[\"Predicted 0\",\"Predicted 0\",\"Predicted 1\",\"Predicted 1\"],\n",
       "\"Count\":[24.0,1.0,2.0,19.0],\n",
       "\"Annotation\":[\"TN: 24\",\"FN: 1\",\"FP: 2\",\"TP: 19\"]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"Predicted\",\n",
       "\"y\":\"Actual\",\n",
       "\"fill\":\"Count\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Actual\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Predicted\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Count\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Annotation\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Confusion Matrix (Train)\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Predicted\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Actual\"\n",
       "}\n",
       "},\n",
       "\"coord\":{\n",
       "\"name\":\"fixed\",\n",
       "\"ratio\":1.0,\n",
       "\"flip\":false\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\",\n",
       "\"legend_position\":\"right\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"#FF7F50\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"geom\":\"text\",\n",
       "\"mapping\":{\n",
       "\"label\":\"Annotation\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"size\":10.0,\n",
       "\"color\":\"black\",\n",
       "\"vjust\":0.5,\n",
       "\"hjust\":0.5,\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"80\"\n",
       "},{\n",
       "\"data\":{\n",
       "\"Actual\":[\"Actual 0\",\"Actual 1\",\"Actual 0\",\"Actual 1\"],\n",
       "\"Predicted\":[\"Predicted 0\",\"Predicted 0\",\"Predicted 1\",\"Predicted 1\"],\n",
       "\"Count\":[5.0,16.0,0.0,0.0],\n",
       "\"Annotation\":[\"TN: 5\",\"FN: 16\",\"FP: 0\",\"TP: 0\"]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"Predicted\",\n",
       "\"y\":\"Actual\",\n",
       "\"fill\":\"Count\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Actual\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Predicted\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Count\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Annotation\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Confusion Matrix (Test)\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Predicted\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Actual\"\n",
       "}\n",
       "},\n",
       "\"coord\":{\n",
       "\"name\":\"fixed\",\n",
       "\"ratio\":1.0,\n",
       "\"flip\":false\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\",\n",
       "\"legend_position\":\"right\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"#FF7F50\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"geom\":\"text\",\n",
       "\"mapping\":{\n",
       "\"label\":\"Annotation\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"size\":10.0,\n",
       "\"color\":\"black\",\n",
       "\"vjust\":0.5,\n",
       "\"hjust\":0.5,\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"81\"\n",
       "}]\n",
       "};\n",
       "               window.letsPlotCall(function() {\n",
       "       \n",
       "               var toolbar = null;\n",
       "               var plotContainer = containerDiv;               \n",
       "               \n",
       "                   var options = {\n",
       "                       sizing: {\n",
       "                           width_mode: \"min\",\n",
       "                           height_mode: \"scaled\",\n",
       "                           width: width\n",
       "                       }\n",
       "                   };\n",
       "                   var fig = LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer, options);\n",
       "                   if (toolbar) {\n",
       "                     toolbar.bind(fig);\n",
       "                   }\n",
       "               });\n",
       "               \n",
       "               break;\n",
       "           }\n",
       "       }\n",
       "   });\n",
       "   \n",
       "   observer.observe(containerDiv);\n",
       "   \n",
       "   // ----------\n",
       "   })();\n",
       "   \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.subplots.SupPlotsSpec at 0x3235affb0>"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Generates and plots a confusion matrix using Lets-Plot.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Actual labels\n",
    "    - y_pred: Predicted labels\n",
    "    - title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Extract TP, FP, FN, TN\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "\n",
    "    # Convert confusion matrix to a DataFrame\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                                  columns=[\"Predicted 0\", \"Predicted 1\"], \n",
    "                                  index=[\"Actual 0\", \"Actual 1\"])\n",
    "\n",
    "    # Melt the confusion matrix DataFrame to long format\n",
    "    conf_matrix_long = conf_matrix_df.reset_index().melt(id_vars=\"index\", value_vars=[\"Predicted 0\", \"Predicted 1\"])\n",
    "    conf_matrix_long.columns = [\"Actual\", \"Predicted\", \"Count\"]\n",
    "\n",
    "    # Define mapping for labels\n",
    "    label_map = {\n",
    "        (\"Actual 0\", \"Predicted 0\"): \"TN\",\n",
    "        (\"Actual 0\", \"Predicted 1\"): \"FP\",\n",
    "        (\"Actual 1\", \"Predicted 0\"): \"FN\",\n",
    "        (\"Actual 1\", \"Predicted 1\"): \"TP\",\n",
    "    }\n",
    "\n",
    "    # Add annotations for TP, FP, FN, TN\n",
    "    conf_matrix_long['Annotation'] = conf_matrix_long.apply(\n",
    "        lambda row: f\"{label_map[(row['Actual'], row['Predicted'])]}: {row['Count']}\", axis=1\n",
    "    )\n",
    "\n",
    "    # Create confusion matrix plot with Lets-Plot\n",
    "    plot = ggplot(conf_matrix_long, aes(x='Predicted', y='Actual', fill='Count')) + \\\n",
    "        geom_tile() + \\\n",
    "        geom_text(aes(label='Annotation'), size=10, color='black', vjust=0.5, hjust=0.5) + \\\n",
    "        scale_fill_gradient(low='white', high='#FF7F50') + \\\n",
    "        ggtitle(title) + \\\n",
    "        xlab('Predicted') + \\\n",
    "        ylab('Actual') + \\\n",
    "        coord_fixed(ratio=1) + \\\n",
    "        theme_minimal() + \\\n",
    "        theme(legend_position='right')\n",
    "\n",
    "    return plot\n",
    "\n",
    "\n",
    "conf_matrix_plot_train = plot_confusion_matrix(y_train, y_train_pred, title=\"Confusion Matrix (Train)\")\n",
    "conf_matrix_plot_test = plot_confusion_matrix(y_test, y_test_pred, title=\"Confusion Matrix (Test)\")\n",
    "\n",
    "gggrid([conf_matrix_plot_train, conf_matrix_plot_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the baseline model classifies the training set well, with a high number of true positives and true negatives. However, the model struggles with the test set. While it is able to correctly classify all interest rate decreases (true negatives), it is also misclassifying all interest rate increases as decreases (false negatives). This is likely due to the class imbalance in the test set, where there are more instances of interest rate increases than decreases. Let's explore this further in 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Interpretation of Regression Coefficients  \n",
    "Here, we see the advantage of using a logistic regression model. It is interpretable, and we can extract the coefficients to understand the impact of each feature on the log-likelihood of the Bank of England raising interest rates. This provides a clear mathematical relationship between the predictors and the target variable, allowing us to understand the direction and strength of the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>-0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cci</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unemployment_rate</td>\n",
       "      <td>-0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gilt_yield</td>\n",
       "      <td>1.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpih</td>\n",
       "      <td>-0.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gva</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>er_gbp_usd</td>\n",
       "      <td>-1.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>er_gbp_eur</td>\n",
       "      <td>-0.856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Coefficient\n",
       "0          Intercept       -0.762\n",
       "1                cci        0.493\n",
       "2  unemployment_rate       -0.185\n",
       "3         gilt_yield        1.916\n",
       "4               cpih       -0.473\n",
       "5                gva        0.195\n",
       "6         er_gbp_usd       -1.428\n",
       "7         er_gbp_eur       -0.856"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get regression coefficients and intercept from the logistic regression model\n",
    "coefficients = np.round(logit.coef_[0], 3)\n",
    "intercept = np.round(logit.intercept_[0], 3)\n",
    "\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': ['Intercept'] + indicators,\n",
    "    'Coefficient': [intercept] + coefficients.tolist()\n",
    "})\n",
    "\n",
    "coefficients_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we standardised all the features, we note that the logistic regression coefficients represent the **log-odds** of the target variable (rate change) given a **one standard deviation increase** in the predictor variable. This is the form of a logistic regression equation:     \n",
    "$$Log Odds = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$$  \n",
    ", where, in our context:\n",
    "* $p$ is the probability of the target variable (rate change) being 1 (interest rate increase)\n",
    "* $\\beta_0$ is the intercept\n",
    "* $\\beta_i$ is the regression coefficient for feature $x_i$\n",
    "\n",
    "\n",
    "Let us look at CCI as an example. The coefficient of ```cci``` is 0.493, which means that for every one standard deviation increase in the CCI, the log-odds of the Bank of England raising the interest rates increases by 0.493, *holding all other variables constant*. This implies that a higher CCI is associated with a higher likelihood of an interest rate increase. For easier interpretation, we can also exponentiate the regression coefficient to get the **odds ratio** for a particular feature $x_i$:\n",
    "$$Odds Ratio_{x_i} = e^{\\beta_i}$$  \n",
    "So continuing with the example of CCI, $Odds Ratio_{cci} = e^{0.493} = 1.637$. This means that for every **one standard deviation increase in the CCI**, the **odds of a rate hike increase by a factor of 1.637**, *holding all other variables constant*. \n",
    "\n",
    ":::{.callout-note}\n",
    "Since there are multiple regressors (features) in the model, the interpretation of the coefficients is based on the assumption that the other variables are held constant. This is known as the **ceteris paribus** assumption.\n",
    ":::\n",
    "  \n",
    "The same interpretation can be applied to the other features in the model, noting that a **negative coefficient** implies that the feature is associated with a **lower likelihood of an interest rate increase**. Each feature's odds ratio is calculated and added to `coefficients_df` below. \n",
    "\n",
    "Lastly, the coefficient of the intercept term has little practical or intuitive meaning in this context, as it represents the log-odds of the target variable when all predictor variables are zero. Since the predictor variables are economic indicators, it is unlikely that they would all be zero at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Odds Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cci</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unemployment_rate</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gilt_yield</td>\n",
       "      <td>1.916</td>\n",
       "      <td>6.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpih</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gva</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>er_gbp_usd</td>\n",
       "      <td>-1.428</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>er_gbp_eur</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Coefficient  Odds Ratio\n",
       "0          Intercept       -0.762       0.467\n",
       "1                cci        0.493       1.637\n",
       "2  unemployment_rate       -0.185       0.831\n",
       "3         gilt_yield        1.916       6.794\n",
       "4               cpih       -0.473       0.623\n",
       "5                gva        0.195       1.215\n",
       "6         er_gbp_usd       -1.428       0.240\n",
       "7         er_gbp_eur       -0.856       0.425"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the odds ratios\n",
    "coefficients_df['Odds Ratio'] = np.exp(coefficients_df['Coefficient']).round(3)\n",
    "\n",
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding the other regressors constant, the 10-year gilt yield (`\"gilt_yield\"`) gives the highest odds ratio of 6.794, indicating that a one standard deviation increase in 10-year gilt yields is associated with a 6.794 times higher odds of the Bank of England raising interest rates. This suggests that the 10-year gilt yield is a strong predictor of the Bank of England's interest rate decisions. \n",
    "\n",
    "### 1.3 Improving the model  \n",
    "In 1.2.1, we noted some issues in the model specifications that could be causing overly optimistic performance metrics. In this section, I will elaborate on them and attempt to improve the baseline logistic regression model by:  \n",
    "1. Applying more appropriate lags to the economic indicators to capture **publication delays**\n",
    "2. Dimensionality reduction using **PCA** to address multicollinearity\n",
    "3. Using a **Random Forest Classifier** to capture non-linear relationships and interactions between features\n",
    "4. **Cross-validation** to ensure robustness of the model (using `TimeSeriesSplit`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, in the baseline model, we did not consider **publication lags**. This refers to the fact that when an economic indicator is published, it typically reflects data from a past period. Based on the sources of each economic indicator in our dataset, we find that the publication delay varies between indicators, as listed below: \n",
    "* [CCI](https://www.oecd.org/en/data/indicators/consumer-confidence-index-cci.html?oecdcontrol-cf46a27224-var1=GBR&oecdcontrol-b2a0dbca4d-var3=1997-01&oecdcontrol-b2a0dbca4d-var4=2024-12), [Unemployment Rate](https://www.ons.gov.uk/employmentandlabourmarket/peoplenotinwork/unemployment/timeseries/mgsx/lms/): 3 months\n",
    "* [GDP (GVA)](https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/gdpmonthlyestimateuktimeseriesdataset), [Monthly Spot Exchange Rates](https://www.bankofengland.co.uk/boeapps/database/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Dec&FY=1974&TD=31&TM=Jan&TY=2025&FNY=&CSVF=TT&html.x=46&html.y=35&C=1D1&C=IN3&Filter=N): 2 months\n",
    "* [10-year Gilt Yield](https://fred.stlouisfed.org/series/IRLTLT01GBM156N#), [CPIH](https://www.ons.gov.uk/economy/inflationandpriceindices/timeseries/l59c/mm23): 1 month  \n",
    "  \n",
    "*What does this mean for our baseline model?*   \n",
    "There was likely severe **data leakage** in the baseline model, where we used the economic indicators from the quarter prior to the rate setting event to predict the rate change. This is a problem because: for example, if the rate setting event was on 06/05/1997, given that we know the data for unemployment rate has a 3-month publication lag, policy makers would not have had access to the unemployment rate for May 1997 at the point of the rate setting event. Instead, they would have used the unemployment rate for February 1997. Hence, in our baseline model, we were actually using data that policy makers would not have had access to at the time of the rate setting event, to predict the rate change. This is a significant issue that will be addressed in our improved model specification.  \n",
    "  \n",
    "*How will we address this in our improved model?*  \n",
    "The code below does the following:\n",
    "1. Check that there are no gaps in the economic indicators data, i.e. that there is data available for each month from the start of the dataset to the end.\n",
    "2. For each economic indicator, we defined its respective publication lag based on the source data.\n",
    "3. For each economic indicator, a 3-month rolling average is computed to reflect the quarter-over-quarter trends policymakers would analyse. \n",
    "4. For each rate setting date (`df_new['Date']`), the code calculates a reference month by subtracting the publication lag from the rate setting date. We then convert this reference month to monthly periods for proper alignment when merging datasets. This is important as we have assumed economic indicators in our dataset to be reported at the start of each month, whereas rate setting events are on specific dates.  \n",
    "5. The quarterly average for each indicator is merged into the main dataset `df_new` based on the reference month, ensuring only historically available data is used for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates: DatetimeIndex([], dtype='datetime64[ns]', freq='MS')\n"
     ]
    }
   ],
   "source": [
    "df_new = df.copy()\n",
    "indicators_df_new = indicators_df.copy()\n",
    "\n",
    "# check that monthly data is available for all months for the economic indicators data\n",
    "date_range = pd.date_range(indicators_df_new['date'].min(), indicators_df_new['date'].max(), freq='MS') \n",
    "missing_dates = date_range[~date_range.isin(indicators_df_new['date'])]\n",
    "print(\"Missing dates:\", missing_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lags for each indicator based on its publication delay\n",
    "indicator_lag = {\n",
    "    'cci': 3,   # this means cci has a 3-month publication delay\n",
    "    'unemployment_rate': 3,\n",
    "    'gilt_yield': 1,  \n",
    "    'cpih': 1,\n",
    "    'gva': 2,\n",
    "    'er_gbp_usd': 2,\n",
    "    'er_gbp_eur': 2\n",
    "}\n",
    "\n",
    "# Process each indicator\n",
    "for indicator, lag in indicator_lag.items():\n",
    "    # Create a temporary DataFrame with 3-month averages\n",
    "    indicator_avg = (\n",
    "        indicators_df_new\n",
    "        .sort_values('date')\n",
    "        .set_index('date')[indicator]\n",
    "        .rolling(window=3, min_periods=3) # min_periods=3 to enforce strict windows (NaN otherwise)\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={indicator: f'{indicator}_avg'})\n",
    "    )\n",
    "    \n",
    "    # For each rate-setting date, compute the reference month (accounting for lag)\n",
    "    df_new[f'{indicator}_ref_month'] = df_new['Date'] - pd.DateOffset(months=lag)\n",
    "    \n",
    "    # Convert reference month to period for alignment when merging\n",
    "    df_new[f'{indicator}_ref_month'] = df_new[f'{indicator}_ref_month'].dt.to_period('M') \n",
    "    indicator_avg['month_period'] = indicator_avg['date'].dt.to_period('M')\n",
    "    \n",
    "    # Merge the 3-month average based on the reference month\n",
    "    df_new = df_new.merge(\n",
    "        indicator_avg,\n",
    "        left_on=f'{indicator}_ref_month',\n",
    "        right_on='month_period',\n",
    "        how='left'\n",
    "    ).rename(columns={f'{indicator}_avg': f'{indicator}_lagged'})\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df_new = df_new.drop(columns=[f'{indicator}_ref_month', 'month_period', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rate</th>\n",
       "      <th>rate_change</th>\n",
       "      <th>Date_diff</th>\n",
       "      <th>cci_lagged</th>\n",
       "      <th>unemployment_rate_lagged</th>\n",
       "      <th>gilt_yield_lagged</th>\n",
       "      <th>cpih_lagged</th>\n",
       "      <th>gva_lagged</th>\n",
       "      <th>er_gbp_usd_lagged</th>\n",
       "      <th>er_gbp_eur_lagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-05-06</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.429533</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>62.366667</td>\n",
       "      <td>0.613767</td>\n",
       "      <td>0.724667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-06-06</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1</td>\n",
       "      <td>31 days</td>\n",
       "      <td>102.491200</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.420167</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>62.800000</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>0.712867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-07-10</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1</td>\n",
       "      <td>34 days</td>\n",
       "      <td>102.671067</td>\n",
       "      <td>7.233333</td>\n",
       "      <td>7.315967</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>62.866667</td>\n",
       "      <td>0.616167</td>\n",
       "      <td>0.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-08-07</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1</td>\n",
       "      <td>28 days</td>\n",
       "      <td>102.803300</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.122533</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.611333</td>\n",
       "      <td>0.698467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-11-06</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>91 days</td>\n",
       "      <td>102.891300</td>\n",
       "      <td>7.066667</td>\n",
       "      <td>6.807267</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>63.466667</td>\n",
       "      <td>0.615667</td>\n",
       "      <td>0.668767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2023-06-22</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>42 days</td>\n",
       "      <td>94.449537</td>\n",
       "      <td>3.966667</td>\n",
       "      <td>3.725400</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>100.533333</td>\n",
       "      <td>0.818267</td>\n",
       "      <td>0.883233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1</td>\n",
       "      <td>42 days</td>\n",
       "      <td>96.307567</td>\n",
       "      <td>4.033333</td>\n",
       "      <td>4.255067</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>100.466667</td>\n",
       "      <td>0.798900</td>\n",
       "      <td>0.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>364 days</td>\n",
       "      <td>99.229860</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.175300</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>101.266667</td>\n",
       "      <td>0.792533</td>\n",
       "      <td>0.853100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-1</td>\n",
       "      <td>98 days</td>\n",
       "      <td>100.100567</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.016333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>101.266667</td>\n",
       "      <td>0.769167</td>\n",
       "      <td>0.845033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2025-02-06</td>\n",
       "      <td>4.50</td>\n",
       "      <td>-1</td>\n",
       "      <td>91 days</td>\n",
       "      <td>99.141300</td>\n",
       "      <td>4.366667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Rate  rate_change Date_diff  cci_lagged  \\\n",
       "0  1997-05-06  6.25            1       NaT         NaN   \n",
       "1  1997-06-06  6.50            1   31 days  102.491200   \n",
       "2  1997-07-10  6.75            1   34 days  102.671067   \n",
       "3  1997-08-07  7.00            1   28 days  102.803300   \n",
       "4  1997-11-06  7.25            1   91 days  102.891300   \n",
       "..        ...   ...          ...       ...         ...   \n",
       "63 2023-06-22  5.00            1   42 days   94.449537   \n",
       "64 2023-08-03  5.25            1   42 days   96.307567   \n",
       "65 2024-08-01  5.00           -1  364 days   99.229860   \n",
       "66 2024-11-07  4.75           -1   98 days  100.100567   \n",
       "67 2025-02-06  4.50           -1   91 days   99.141300   \n",
       "\n",
       "    unemployment_rate_lagged  gilt_yield_lagged  cpih_lagged  gva_lagged  \\\n",
       "0                        NaN           7.429533     0.266667   62.366667   \n",
       "1                   7.333333           7.420167     0.333333   62.800000   \n",
       "2                   7.233333           7.315967     0.333333   62.866667   \n",
       "3                   7.200000           7.122533     0.133333   63.000000   \n",
       "4                   7.066667           6.807267     0.266667   63.466667   \n",
       "..                       ...                ...          ...         ...   \n",
       "63                  3.966667           3.725400     0.833333  100.533333   \n",
       "64                  4.033333           4.255067     0.166667  100.466667   \n",
       "65                  4.333333           4.175300     0.200000  101.266667   \n",
       "66                  4.200000           4.016333     0.366667  101.266667   \n",
       "67                  4.366667                NaN          NaN         NaN   \n",
       "\n",
       "    er_gbp_usd_lagged  er_gbp_eur_lagged  \n",
       "0            0.613767           0.724667  \n",
       "1            0.617300           0.712867  \n",
       "2            0.616167           0.707600  \n",
       "3            0.611333           0.698467  \n",
       "4            0.615667           0.668767  \n",
       "..                ...                ...  \n",
       "63           0.818267           0.883233  \n",
       "64           0.798900           0.869900  \n",
       "65           0.792533           0.853100  \n",
       "66           0.769167           0.845033  \n",
       "67                NaN                NaN  \n",
       "\n",
       "[68 rows x 11 columns]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have addressed the issue of publication lags, we will also do the following: \n",
    "1. Encode the binary target variable `rate_change` as 0 for a rate decrease and 1 for a rate increase.\n",
    "2. Drop the `Date_diff` column as it is not needed for this analysis.\n",
    "3. Drop rows with missing values in the dataset (more on this below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the rate_change column\n",
    "df_new['rate_change'] = (df_new['rate_change'] == 1).astype(int)\n",
    "\n",
    "# drop the Date_diff column\n",
    "df_new = df_new.drop('Date_diff', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the only rows with missing values are the first and last rows of the dataset. The first row is missing values because there are no data for CCI and unemployment rate available for the quarter prior to the first rate setting event in the dataset. The last row is missing values because there are no data for 10-year gilt yields, CPIH, GVA, and monthly spot exchange rates for the quarter prior to the last rate setting event in the dataset. It also makes sense that there are no values missing for the years in between, as we have previously checked that there are no gaps in the economic indicators data.  \n",
    "  \n",
    "Since these rows do not contain any information that can be used for prediction, we drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                        0\n",
      "Rate                        0\n",
      "rate_change                 0\n",
      "cci_lagged                  1\n",
      "unemployment_rate_lagged    1\n",
      "gilt_yield_lagged           1\n",
      "cpih_lagged                 1\n",
      "gva_lagged                  1\n",
      "er_gbp_usd_lagged           1\n",
      "er_gbp_eur_lagged           1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_new.isnull().sum())\n",
    "\n",
    "# drop rows with missing lag values\n",
    "df_new = df_new.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is easy to suspect that the economic indicators are correlated with each other, which could lead to multicollinearity in the model. We can plot a correlation matrix to visualise the relationships between the economic indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"KtYSn1\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "   \n",
       "   (function() {\n",
       "   // ----------\n",
       "   \n",
       "   var containerDiv = document.getElementById(\"KtYSn1\");\n",
       "   var observer = new ResizeObserver(function(entries) {\n",
       "       for (let entry of entries) {\n",
       "           var width = containerDiv.clientWidth\n",
       "           if (entry.contentBoxSize && width > 0) {\n",
       "           \n",
       "               // Render plot\n",
       "               if (observer) {\n",
       "                   observer.disconnect();\n",
       "                   observer = null;\n",
       "               }\n",
       "\n",
       "               var plotSpec={\n",
       "\"data\":{\n",
       "\"index\":[\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\"],\n",
       "\"variable\":[\"cci_lagged\",\"cci_lagged\",\"cci_lagged\",\"cci_lagged\",\"cci_lagged\",\"cci_lagged\",\"cci_lagged\",\"unemployment_rate_lagged\",\"unemployment_rate_lagged\",\"unemployment_rate_lagged\",\"unemployment_rate_lagged\",\"unemployment_rate_lagged\",\"unemployment_rate_lagged\",\"unemployment_rate_lagged\",\"gilt_yield_lagged\",\"gilt_yield_lagged\",\"gilt_yield_lagged\",\"gilt_yield_lagged\",\"gilt_yield_lagged\",\"gilt_yield_lagged\",\"gilt_yield_lagged\",\"cpih_lagged\",\"cpih_lagged\",\"cpih_lagged\",\"cpih_lagged\",\"cpih_lagged\",\"cpih_lagged\",\"cpih_lagged\",\"gva_lagged\",\"gva_lagged\",\"gva_lagged\",\"gva_lagged\",\"gva_lagged\",\"gva_lagged\",\"gva_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_usd_lagged\",\"er_gbp_eur_lagged\",\"er_gbp_eur_lagged\",\"er_gbp_eur_lagged\",\"er_gbp_eur_lagged\",\"er_gbp_eur_lagged\",\"er_gbp_eur_lagged\",\"er_gbp_eur_lagged\"],\n",
       "\"value\":[1.0,0.4941020672496486,0.36270215490804364,-0.4003842497872192,-0.6754657581801561,-0.5129017983670773,-0.7565747992462767,0.4941020672496486,1.0,0.7540119842395421,-0.44946436530371264,-0.8669772758171124,-0.6698816399154496,-0.5791869327854395,0.36270215490804364,0.7540119842395421,1.0,-0.3139777230389107,-0.7954814371575276,-0.5581410407609247,-0.7113362776605415,-0.4003842497872192,-0.44946436530371264,-0.3139777230389107,1.0,0.4923414513052729,0.4036124143060673,0.41064234706206665,-0.6754657581801561,-0.8669772758171124,-0.7954814371575276,0.4923414513052729,1.0,0.6313466140069521,0.8509002187084911,-0.5129017983670773,-0.6698816399154496,-0.5581410407609247,0.4036124143060673,0.6313466140069521,1.0,0.6335475105920455,-0.7565747992462767,-0.5791869327854395,-0.7113362776605415,0.41064234706206665,0.8509002187084911,0.6335475105920455,1.0]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"index\",\n",
       "\"y\":\"variable\",\n",
       "\"fill\":\"value\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"index\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"variable\"\n",
       "},{\n",
       "\"type\":\"float\",\n",
       "\"column\":\"value\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Correlation Matrix of Economic Indicators (Lagged)\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Indicator\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Indicator\"\n",
       "}\n",
       "},\n",
       "\"theme\":{\n",
       "\"axis_text_x\":{\n",
       "\"angle\":45.0,\n",
       "\"hjust\":1.0,\n",
       "\"blank\":false\n",
       "}\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":800.0,\n",
       "\"height\":800.0\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"blue\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"82\"\n",
       "};\n",
       "               window.letsPlotCall(function() {\n",
       "       \n",
       "               var toolbar = null;\n",
       "               var plotContainer = containerDiv;               \n",
       "               \n",
       "                   var options = {\n",
       "                       sizing: {\n",
       "                           width_mode: \"min\",\n",
       "                           height_mode: \"scaled\",\n",
       "                           width: width\n",
       "                       }\n",
       "                   };\n",
       "                   var fig = LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer, options);\n",
       "                   if (toolbar) {\n",
       "                     toolbar.bind(fig);\n",
       "                   }\n",
       "               });\n",
       "               \n",
       "               break;\n",
       "           }\n",
       "       }\n",
       "   });\n",
       "   \n",
       "   observer.observe(containerDiv);\n",
       "   \n",
       "   // ----------\n",
       "   })();\n",
       "   \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x3235afd70>"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation matrix\n",
    "indicators_lagged_cols = [f'{indicator}_lagged' for indicator in indicators]\n",
    "numeric_df = df_new[indicators_lagged_cols]\n",
    "indicators_corr = numeric_df.corr()\n",
    "\n",
    "# plot the correlation matrix\n",
    "ggplot(pd.melt(indicators_corr.reset_index(), id_vars='index'), aes(x='index', y='variable', fill='value')) + \\\n",
    "    geom_tile() + \\\n",
    "    scale_fill_gradient(low='white', high='blue') + \\\n",
    "    labs(title='Correlation Matrix of Economic Indicators (Lagged)', x='Indicator', y='Indicator') + \\\n",
    "    theme(axis_text_x=element_text(angle=45, hjust=1)) + \\\n",
    "    ggsize(800, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that **many indicators are moderately to highly correlated with each other**- I will elaborate on two examples:\n",
    "* GDP (`\"gva_lagged\"`) and unemployment rate (`\"unemployment_rate_lagged\"`) are highly correlated with each other, with a correlation coefficient of -0.867.\n",
    "    + This aligns with basic economic theory- when the economy is growing (GDP is rising), businesses tend to expand, leading to increased demand for labour and thus lower unemployment. \n",
    "* The monthly spot exchange rate for GBP to EUR (`\"er_gbp_eur_lagged\"`) and GDP are also highly correlated, with a correlation coefficient of 0.851. \n",
    "    + Strong economic performance in the UK (higher GDP) attracts foreign investment in UK assets, increasing the demand for GBP and thus appreciating its value against the EUR.  \n",
    "  \n",
    "To address multicollinearity, we can use **PCA to reduce the dimensionality** of the dataset while retaining as much information as possible. PCA will transform the original economic indicators into a set of linearly uncorrelated variables called principal components. These principal components are ordered by the amount of variance they explain in the data, with the first component explaining the most variance. By selecting a subset of the principal components that explain most of the variance in the data, we can reduce the dimensionality of the dataset and mitigate multicollinearity. \n",
    "  \n",
    "Taking a few steps back, let us first split the data into training and test sets, using the same method of splitting as in the baseline model (70% of the years for training set). Again, we use a `train_test_split` with `shuffle=False` to maintain the chronological order of the data, as we are dealing with time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_new[indicators_lagged_cols], df_new['rate_change']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, our test set is quite imbalanced, with 80% of the instances being rate increases (class 1): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Distribution: \n",
      "rate_change\n",
      "0    0.586957\n",
      "1    0.413043\n",
      "Name: proportion, dtype: float64\n",
      "Test Class Distribution: \n",
      "rate_change\n",
      "1    0.8\n",
      "0    0.2\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Class Distribution: \\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test Class Distribution: \\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 20)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One preprocessing step we take is to **standardise the features** before applying PCA so that all features have a mean of 0 and a standard deviation of 1. This step ensures that each feature contributes equally to the PCA, since PCA is sensitive to the scale of the features.  \n",
    "  \n",
    "We will use a `Pipeline` to combine the standardisation, PCA, and instantiation of a Random Forest Classifier into a single object. This allows us to streamline the process and ensures that the same transformations are applied to both the training and test sets. Also note that:  \n",
    "* `n_components` parameter in PCA is set to 0.90, meaning that PCA will retain enough components to explain 90% of the variance in the data. We want to retain as much information as possible while reducing the dimensionality of the dataset.  \n",
    "* `class_weight='balanced'` parameter in the Random Forest Classifier is used to account for the slight class imbalance in the training set when we fit the pipeline to the training data. This parameter assigns weights to the classes inversely proportional to their frequency, so that the model gives more importance to the minority class (rate increases) during training.\n",
    "* `TimeSeriesSplit` is used to ensure that the cross-validation preserves the temporal order of the data. This is important as we are dealing with time series data, and we want to avoid data leakage from the future into the past. `n_splits` is set to 3, i.e. the training data is split into 3 folds for cross-validation, as we have a *limited amount of data* (46 data points in the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/3...\n",
      "Processing fold 2/3...\n",
      "Processing fold 3/3...\n",
      "Average Train AUC-PR: 1.000\n",
      "Average Validation AUC-PR: 0.561\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.9)),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "num_folds = tscv.get_n_splits()\n",
    "\n",
    "# lists to store metrics and confusion matrices\n",
    "train_results, val_results = [], []\n",
    "train_cm_sum= None\n",
    "\n",
    "# cross-validation loop on the training set only\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    print(f\"Processing fold {fold}/{num_folds}...\")\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Fit the pipeline only on the training set\n",
    "    pipeline.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # predictions\n",
    "    y_train_pred = pipeline.predict(X_train_fold)\n",
    "    y_val_pred = pipeline.predict(X_val_fold)\n",
    "    y_train_probas = pipeline.predict_proba(X_train_fold)[:, 1]\n",
    "    y_val_probas = pipeline.predict_proba(X_val_fold)[:, 1]  \n",
    "\n",
    "    # Calculate the AUC-PR\n",
    "    train_auc_pr = average_precision_score(y_train_fold, y_train_probas)\n",
    "    val_auc_pr = average_precision_score(y_val_fold, y_val_probas)\n",
    "\n",
    "    # store the results\n",
    "    train_results.append(train_auc_pr)\n",
    "    val_results.append(val_auc_pr)\n",
    "\n",
    "    # compute confusion matrices\n",
    "    labels = np.sort(y.unique())\n",
    "    cm_train = confusion_matrix(y_train_fold, y_train_pred, labels=labels)\n",
    "\n",
    "    # sum the confusion matrices\n",
    "    train_cm_sum = cm_train if train_cm_sum is None else train_cm_sum + cm_train\n",
    "\n",
    "# calculate the average AUC-PR for the training and validation sets\n",
    "print(f\"Average Train AUC-PR: {np.mean(train_results):.3f}\")\n",
    "print(f\"Average Validation AUC-PR: {np.mean(val_results):.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the cross-validation on the training set and obtaining its aggregate AUC-PR score, we fit the pipeline to the entire training set and evaluate the model on the test set. This allows us to assess the model's performance on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance: \n",
      "AUC-PR = 0.936\n"
     ]
    }
   ],
   "source": [
    "_ = pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "y_test_probas = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calculate the performance metrics\n",
    "auc_pr = average_precision_score(y_test, y_test_probas)\n",
    "\n",
    "print(f\"Test Set Performance: \\nAUC-PR = {auc_pr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Interpretation of Random Forest Classifier Performance (AUC-PR)  \n",
    "|AUC-PR|Baseline Logistic Regression|Improved Random Forest Classifier|\n",
    "|---|---|---|\n",
    "|Train set|0.972|1.000|\n",
    "|Test set|0.734|0.936|  \n",
    "  \n",
    "Based on the AUC-PR metric, it is clear that the improved Random Forest Classifier outperforms the baseline logistic regression model on both the training and test sets. The Random Forest Classifier achieves a perfect AUC-PR of 1.000 on the training set, indicating that it can effectively distinguish between rate increases and decreases in the training data. On the test set, the Random Forest Classifier achieves an AUC-PR of 0.936, which is significantly higher than the baseline model's AUC-PR of 0.734 (albeit still a **red flag for overfitting**).  \n",
    "  \n",
    "Therefore, the steps that we have taken to address the issues in the baseline model- accounting for publication lags, reducing dimensionality using PCA, and using a Random Forest Classifier with balanced class weights- have led to a clear **improvement in the model's predictive performance**.  \n",
    "  \n",
    "#### 1.3.2 Confusion Matrices  \n",
    "We draw the (averaged) confusion matrices over all cross validation folds in the training set, as well as the confusion matrix for the test set to once again get a visual representation of the model's performance in terms of true positives, false positives, true negatives, and false negatives.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"1lOUgh\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "   \n",
       "   (function() {\n",
       "   // ----------\n",
       "   \n",
       "   var containerDiv = document.getElementById(\"1lOUgh\");\n",
       "   var observer = new ResizeObserver(function(entries) {\n",
       "       for (let entry of entries) {\n",
       "           var width = containerDiv.clientWidth\n",
       "           if (entry.contentBoxSize && width > 0) {\n",
       "           \n",
       "               // Render plot\n",
       "               if (observer) {\n",
       "                   observer.disconnect();\n",
       "                   observer = null;\n",
       "               }\n",
       "\n",
       "               var plotSpec={\n",
       "\"kind\":\"subplots\",\n",
       "\"layout\":{\n",
       "\"ncol\":2.0,\n",
       "\"nrow\":1.0,\n",
       "\"widths\":[1.0,1.0],\n",
       "\"name\":\"grid\"\n",
       "},\n",
       "\"figures\":[{\n",
       "\"data\":{\n",
       "\"Actual\":[0.0,0.0,1.0,1.0],\n",
       "\"Predicted\":[0.0,1.0,0.0,1.0],\n",
       "\"Count\":[13.0,0.0,0.0,11.0],\n",
       "\"Annotation\":[\"TN: 13\",\"FP: 0\",\"FN: 0\",\"TP: 11\"]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"Predicted\",\n",
       "\"y\":\"Actual\",\n",
       "\"fill\":\"Count\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Actual\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Predicted\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Count\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Dataset\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Annotation\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Averaged Confusion Matrix (Train - CV)\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Predicted\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Actual\"\n",
       "}\n",
       "},\n",
       "\"coord\":{\n",
       "\"name\":\"fixed\",\n",
       "\"ratio\":1.0,\n",
       "\"flip\":false\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\",\n",
       "\"legend_position\":\"right\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"#FF7F50\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"geom\":\"text\",\n",
       "\"mapping\":{\n",
       "\"label\":\"Annotation\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"size\":10.0,\n",
       "\"color\":\"black\",\n",
       "\"vjust\":0.5,\n",
       "\"hjust\":0.5,\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"83\"\n",
       "},{\n",
       "\"data\":{\n",
       "\"Actual\":[\"Actual 0\",\"Actual 1\",\"Actual 0\",\"Actual 1\"],\n",
       "\"Predicted\":[\"Predicted 0\",\"Predicted 0\",\"Predicted 1\",\"Predicted 1\"],\n",
       "\"Count\":[4.0,16.0,0.0,0.0],\n",
       "\"Annotation\":[\"TN: 4\",\"FN: 16\",\"FP: 0\",\"TP: 0\"]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"Predicted\",\n",
       "\"y\":\"Actual\",\n",
       "\"fill\":\"Count\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Actual\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Predicted\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Count\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Annotation\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Confusion Matrix (Test)\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Predicted\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Actual\"\n",
       "}\n",
       "},\n",
       "\"coord\":{\n",
       "\"name\":\"fixed\",\n",
       "\"ratio\":1.0,\n",
       "\"flip\":false\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\",\n",
       "\"legend_position\":\"right\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"#FF7F50\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"geom\":\"text\",\n",
       "\"mapping\":{\n",
       "\"label\":\"Annotation\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"size\":10.0,\n",
       "\"color\":\"black\",\n",
       "\"vjust\":0.5,\n",
       "\"hjust\":0.5,\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"84\"\n",
       "}]\n",
       "};\n",
       "               window.letsPlotCall(function() {\n",
       "       \n",
       "               var toolbar = null;\n",
       "               var plotContainer = containerDiv;               \n",
       "               \n",
       "                   var options = {\n",
       "                       sizing: {\n",
       "                           width_mode: \"min\",\n",
       "                           height_mode: \"scaled\",\n",
       "                           width: width\n",
       "                       }\n",
       "                   };\n",
       "                   var fig = LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer, options);\n",
       "                   if (toolbar) {\n",
       "                     toolbar.bind(fig);\n",
       "                   }\n",
       "               });\n",
       "               \n",
       "               break;\n",
       "           }\n",
       "       }\n",
       "   });\n",
       "   \n",
       "   observer.observe(containerDiv);\n",
       "   \n",
       "   // ----------\n",
       "   })();\n",
       "   \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.subplots.SupPlotsSpec at 0x3235ad430>"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute mean confusion matrix for the training set since we have multiple folds\n",
    "train_cm_avg = np.round(train_cm_sum / num_folds).astype(int)\n",
    "\n",
    "# convert confusion matrices to dataframe\n",
    "def get_cm_df(cm, dataset, labels):\n",
    "    \"\"\"\n",
    "    Converts a confusion matrix to a DataFrame for plotting.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: Confusion matrix (2D numpy array)\n",
    "    - dataset: Label for the dataset (e.g., 'Train' or 'Test')\n",
    "    - labels: Class labels\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns: ['Actual', 'Predicted', 'Count', 'Dataset']\n",
    "    \"\"\"\n",
    "    return pd.DataFrame([\n",
    "        (actual, predicted, cm[i, j]) for i, actual in enumerate(labels) for j, predicted in enumerate(labels)\n",
    "    ], columns=[\"Actual\", \"Predicted\", \"Count\"]).assign(Dataset=dataset)\n",
    "\n",
    "train_cm_df = get_cm_df(train_cm_avg, 'Train (CV-Averaged)', labels)\n",
    "\n",
    "# Define the label map for annotations\n",
    "label_map = {\n",
    "    (0, 0): \"TN\",\n",
    "    (0, 1): \"FP\",\n",
    "    (1, 0): \"FN\",\n",
    "    (1, 1): \"TP\",\n",
    "}\n",
    "\n",
    "def plot_confusion_matrix_with_annotations(cm_df, label_map, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Creates and plots a confusion matrix with annotations for TP, FP, FN, TN using Lets-Plot.\n",
    "\n",
    "    Parameters:\n",
    "    - cm_df: DataFrame containing confusion matrix details\n",
    "    - label_map: Dictionary mapping (Actual, Predicted) to annotation labels\n",
    "    - title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Add annotations for TP, FP, FN, TN\n",
    "    cm_df['Annotation'] = cm_df.apply(\n",
    "        lambda row: f\"{label_map[(row['Actual'], row['Predicted'])]}: {row['Count']}\", axis=1\n",
    "    )\n",
    "\n",
    "    # Create confusion matrix plot with Lets-Plot\n",
    "    plot = ggplot(cm_df, aes(x=\"Predicted\", y=\"Actual\", fill=\"Count\")) + \\\n",
    "        geom_tile() + \\\n",
    "        geom_text(aes(label=\"Annotation\"), size=10, color=\"black\", vjust=0.5, hjust=0.5) + \\\n",
    "        scale_fill_gradient(low=\"white\", high=\"#FF7F50\") + \\\n",
    "        ggtitle(title) + \\\n",
    "        xlab(\"Predicted\") + \\\n",
    "        ylab(\"Actual\") + \\\n",
    "        coord_fixed(ratio=1) + \\\n",
    "        theme_minimal() + \\\n",
    "        theme(legend_position=\"right\")\n",
    "\n",
    "    return plot\n",
    "\n",
    "# Now plot the confusion matrix with annotations\n",
    "p_train_avg_with_annotations = plot_confusion_matrix_with_annotations(train_cm_df, label_map, title=\"Averaged Confusion Matrix (Train - CV)\")\n",
    "p_test = plot_confusion_matrix(y_test, y_test_pred, title=\"Confusion Matrix (Test)\")\n",
    "\n",
    "gggrid([p_train_avg_with_annotations, p_test], ncol=2, widths=[1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier perfectly classifies the training set even with cross-validation, showing an improved performance compared to the baseline logistic regression. However, it **still fails to correctly classify any interest rate increases in the test set** (false negatives), even as the test set itself exhibits a relatively high proportion (80%) of rate increases.   \n",
    "  \n",
    "One potential explanation is that the **unprecedented economic events** in recent years, such as the pandemic and the subsequent monetary policy responses, introduced market dynamics that were not present in earlier training data. Consequently, the model, even after incorporating publication lags, PCA, and other preprocessing steps, is unable to capture the new patterns associated with rate hikes. Essentially, the model’s predicted probabilities for rate hikes remain extremely low (mean probability = 0.0665), leading to a default classification of \"rate decrease\" when using the standard threshold. Let's look at the summary statistics of the predicted probabilities for the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.02\n",
      "Max: 0.2\n",
      "Mean: 0.0665\n",
      "Median: 0.03\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", np.min(y_test_probas))\n",
    "print(\"Max:\", np.max(y_test_probas))\n",
    "print(\"Mean:\", np.mean(y_test_probas))\n",
    "print(\"Median:\", np.median(y_test_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that, **while the model might rank instances relatively well** (as indicated by a high AUC-PR in the test set), its calibration is off - it is **too conservative in assigning higher probabilities to rate hikes**. Addressing this discrepancy may require a more targeted rebalancing of the training data, threshold recalibration, or the incorporation of additional features that capture these unprecedented economic conditions. I explore one of these approaches in 1.3.3 as an extension.  \n",
    " \n",
    "#### 1.3.3 Rebalancing the Train/Test Split\n",
    "Instead of the instructed 70/30 split, let's consider a **75/25 split** for the training and test sets instead. The motivation behind this is due to the fact that in our baseline analysis, the training set largely consisted of pre-2021 observations when rate hikes were relatively rare. This led the model to learn patterns indicating a low likelihood of rate increases. By shifting to an 75/25 split, we ensure that more recent data—where rate hikes have become more frequent—is included in the training set. The aim is to provide the model with up-to-date economic signals reflective of the current monetary policy environment, potentially leading to more realistic probability estimates and improved predictive performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_new[indicators_lagged_cols], df_new['rate_change']\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, y, test_size=0.25, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Distribution: \n",
      "rate_change\n",
      "0    28\n",
      "1    21\n",
      "Name: count, dtype: int64\n",
      "Test Class Distribution: \n",
      "rate_change\n",
      "1    14\n",
      "0     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Class Distribution: \\n{y_train_new.value_counts()}\")\n",
    "print(f\"Test Class Distribution: \\n{y_test_new.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/3...\n",
      "Processing fold 2/3...\n",
      "Processing fold 3/3...\n",
      "Average Train AUC-PR: 1.000\n",
      "Average Validation AUC-PR: 0.387\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.9)),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "num_folds = tscv.get_n_splits()\n",
    "\n",
    "# lists to store metrics and confusion matrices\n",
    "train_results_new, val_results_new = [], []\n",
    "\n",
    "# cross-validation loop on the training set only\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_new), 1):\n",
    "    print(f\"Processing fold {fold}/{num_folds}...\")\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_new_fold, X_val_new_fold = X_train_new.iloc[train_idx], X_train_new.iloc[val_idx]\n",
    "    y_train_new_fold, y_val_new_fold = y_train_new.iloc[train_idx], y_train_new.iloc[val_idx]\n",
    "\n",
    "    # Fit the pipeline only on the training set\n",
    "    pipeline.fit(X_train_new_fold, y_train_new_fold)\n",
    "\n",
    "    # predictions\n",
    "    y_train_new_pred = pipeline.predict(X_train_new_fold)\n",
    "    y_val_new_pred = pipeline.predict(X_val_new_fold)\n",
    "    y_train_new_probas = pipeline.predict_proba(X_train_new_fold)[:, 1]\n",
    "    y_val_new_probas = pipeline.predict_proba(X_val_new_fold)[:, 1]  \n",
    "\n",
    "    # Calculate the AUC-PR\n",
    "    train_auc_pr_new = average_precision_score(y_train_new_fold, y_train_new_probas)\n",
    "    val_auc_pr_new = average_precision_score(y_val_new_fold, y_val_new_probas)\n",
    "\n",
    "    # store the results\n",
    "    train_results_new.append(train_auc_pr_new)\n",
    "    val_results_new.append(val_auc_pr_new)\n",
    "\n",
    "# calculate the average AUC-PR for the training and validation sets\n",
    "print(f\"Average Train AUC-PR: {np.mean(train_results_new):.3f}\")\n",
    "print(f\"Average Validation AUC-PR: {np.mean(val_results_new):.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance: \n",
      "AUC-PR = 0.970\n"
     ]
    }
   ],
   "source": [
    "_ = pipeline.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_test_pred_new = pipeline.predict(X_test_new)\n",
    "y_test_probas_new = pipeline.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# calculate the performance metrics\n",
    "auc_pr_new = average_precision_score(y_test_new, y_test_probas_new)\n",
    "\n",
    "print(f\"Test Set Performance: \\nAUC-PR = {auc_pr_new:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.12\n",
      "Max: 0.83\n",
      "Mean: 0.6611764705882353\n",
      "Median: 0.69\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", np.min(y_test_probas_new))\n",
    "print(\"Max:\", np.max(y_test_probas_new))\n",
    "print(\"Mean:\", np.mean(y_test_probas_new))\n",
    "print(\"Median:\", np.median(y_test_probas_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"ENqGgD\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "   \n",
       "   (function() {\n",
       "   // ----------\n",
       "   \n",
       "   var containerDiv = document.getElementById(\"ENqGgD\");\n",
       "   var observer = new ResizeObserver(function(entries) {\n",
       "       for (let entry of entries) {\n",
       "           var width = containerDiv.clientWidth\n",
       "           if (entry.contentBoxSize && width > 0) {\n",
       "           \n",
       "               // Render plot\n",
       "               if (observer) {\n",
       "                   observer.disconnect();\n",
       "                   observer = null;\n",
       "               }\n",
       "\n",
       "               var plotSpec={\n",
       "\"data\":{\n",
       "\"Actual\":[\"Actual 0\",\"Actual 1\",\"Actual 0\",\"Actual 1\"],\n",
       "\"Predicted\":[\"Predicted 0\",\"Predicted 0\",\"Predicted 1\",\"Predicted 1\"],\n",
       "\"Count\":[1.0,1.0,2.0,13.0],\n",
       "\"Annotation\":[\"TN: 1\",\"FN: 1\",\"FP: 2\",\"TP: 13\"]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"Predicted\",\n",
       "\"y\":\"Actual\",\n",
       "\"fill\":\"Count\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "\"series_annotations\":[{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Actual\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Predicted\"\n",
       "},{\n",
       "\"type\":\"int\",\n",
       "\"column\":\"Count\"\n",
       "},{\n",
       "\"type\":\"str\",\n",
       "\"column\":\"Annotation\"\n",
       "}]\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Confusion Matrix (Test) (75/25 Split)\"\n",
       "},\n",
       "\"guides\":{\n",
       "\"x\":{\n",
       "\"title\":\"Predicted\"\n",
       "},\n",
       "\"y\":{\n",
       "\"title\":\"Actual\"\n",
       "}\n",
       "},\n",
       "\"coord\":{\n",
       "\"name\":\"fixed\",\n",
       "\"ratio\":1.0,\n",
       "\"flip\":false\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"minimal\",\n",
       "\"legend_position\":\"right\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"#FF7F50\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "}\n",
       "},{\n",
       "\"geom\":\"text\",\n",
       "\"mapping\":{\n",
       "\"label\":\"Annotation\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"size\":10.0,\n",
       "\"color\":\"black\",\n",
       "\"vjust\":0.5,\n",
       "\"hjust\":0.5,\n",
       "\"data\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[],\n",
       "\"spec_id\":\"85\"\n",
       "};\n",
       "               window.letsPlotCall(function() {\n",
       "       \n",
       "               var toolbar = null;\n",
       "               var plotContainer = containerDiv;               \n",
       "               \n",
       "                   var options = {\n",
       "                       sizing: {\n",
       "                           width_mode: \"min\",\n",
       "                           height_mode: \"scaled\",\n",
       "                           width: width\n",
       "                       }\n",
       "                   };\n",
       "                   var fig = LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer, options);\n",
       "                   if (toolbar) {\n",
       "                     toolbar.bind(fig);\n",
       "                   }\n",
       "               });\n",
       "               \n",
       "               break;\n",
       "           }\n",
       "       }\n",
       "   });\n",
       "   \n",
       "   observer.observe(containerDiv);\n",
       "   \n",
       "   // ----------\n",
       "   })();\n",
       "   \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x3235adf70>"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test_new = plot_confusion_matrix(y_test_new, y_test_pred_new, title=\"Confusion Matrix (Test) (75/25 Split)\")\n",
    "p_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results**: \n",
    "Now, the Random Forest Classifier almost perfectly classfies rate hikes in the test set (only 1 false negative), a significant improvement from the previous model. However, the number of false positives also increased. This suggests that the model is now more sensitive to rate hikes, but at the cost of making more false positive predictions.  \n",
    "  \n",
    "Comparing the AUC-PR scores: \n",
    "| AUC-PR | Random Forest Classifier (70/30 split) | Improved Random Forest Classifier (75/25 split) |\n",
    "|--------|---------------------------------------|-------------------------------------------------|\n",
    "| Aggregate Train | 1.000 | 1.000 |\n",
    "| Aggregate Validation | 0.561 | 0.387 |\n",
    "| Test set | 0.936 | 0.970 |  \n",
    "  \n",
    "The discrepency between the aggregate train and validation AUC-PR scores in the 75/25 split model is even greater now as compared to the model with a 70/30 split. Despite the overfitting evident in cross-validation, the test set AUC-PR remains high (0.970) and is close to the training performance. This may be because the test set—comprising more recent data—has a distribution of economic indicators that the model, having been trained on more recent patterns (due to the 75/25 split), can rank effectively. In other words, while the model struggles with certain periods during cross-validation, it appears to perform well on the test period, likely because the test data reflects conditions similar to those the model overfit on. As such, it is difficult to conclude whether this new model truly generalises better to unseen data. \n",
    "  \n",
    "### 1.4 Conclusion \n",
    "Predicting interest rate hikes and cuts is inherently challenging due to the complex and dynamic nature of economic conditions. In the real world, central banks base their decisions on a variety of factors, including inflation, unemployment, geopolitical events, and economic shocks, all of which are often unpredictable. The difficulty in accurately predicting these rate changes is compounded by the fact that these events are rare and can be heavily influenced by unforeseen circumstances, such as the COVID-19 pandemic or geopolitical tensions like the Russia-Ukraine war. \n",
    "\n",
    "Moreover, models trained on historical data may struggle to predict future rate changes if they encounter new economic patterns or events that weren't present in the training data. This is evident in our analysis, where even with an improved model, we saw limitations in its ability to predict rate hikes, especially in the post-2021 period, when monetary policy became more reactive to unforeseen challenges. \n",
    "\n",
    "Thus, while machine learning models can provide insights and offer probability estimates, they face significant limitations when it comes to capturing the full range of variables and shocks that influence central bank decisions.   \n",
    "  \n",
    "## Generative AI acknowledgement  \n",
    "This notebook was written on VSCode with the GitHub Copilot extension activated. It provided some useful suggestions when autocompleting code snippets. I also used ChatGPT for debugging and for checking my understanding and interpretations of the evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS202W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
